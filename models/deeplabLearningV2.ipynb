{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1327578,"sourceType":"datasetVersion","datasetId":767686},{"sourceId":1327590,"sourceType":"datasetVersion","datasetId":769463}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport nibabel as nib\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"‚úÖ All packages installed and imported!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:55:21.341411Z","iopub.execute_input":"2025-11-30T18:55:21.341763Z","iopub.status.idle":"2025-11-30T18:55:22.258066Z","shell.execute_reply.started":"2025-11-30T18:55:21.341735Z","shell.execute_reply":"2025-11-30T18:55:22.257116Z"}},"outputs":[{"name":"stderr","text":"\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n    ColabKernelApp.launch_instance()\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n    res = shell.run_cell(\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n    result = self._run_cell(\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_183/3482444662.py\", line 5, in <cell line: 0>\n    import matplotlib.pyplot as plt\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n    from . import _api, _version, cbook, _docstring, rcsetup\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n    from matplotlib.colors import Colormap, is_color_like\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n    from matplotlib import _api, _cm, cbook, scale\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n    from matplotlib.ticker import (\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n    from matplotlib import transforms as mtransforms\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n    from matplotlib._path import (\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"],"ename":"AttributeError","evalue":"_ARRAY_API not found","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_183/3482444662.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnibabel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# cbook must import matplotlib only within function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m# definitions, so it is safe to import from it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_docstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcsetup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msanitize_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMatplotlibDeprecationWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mls_mapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColormap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_color_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fontconfig_pattern\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_fontconfig_pattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enums\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJoinStyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCapStyle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_color_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBASE_COLORS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTABLEAU_COLORS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCSS4_COLORS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXKCD_COLORS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from matplotlib.ticker import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mNullFormatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mScalarFormatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogFormatterSciNotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogitFormatter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mNullLocator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogLocator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoLocator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoMinorLocator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0m_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m from matplotlib._path import (\n\u001b[0m\u001b[1;32m     50\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"],"ename":"ImportError","evalue":"numpy.core.multiarray failed to import","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"!pip install segmentation-models-pytorch\n!pip install torchvision\n!pip install opencv-python\n!pip install nibabel\n!pip install tqdm\n!pip install scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:55:23.255179Z","iopub.execute_input":"2025-11-30T18:55:23.255411Z","iopub.status.idle":"2025-11-30T18:55:42.108109Z","shell.execute_reply.started":"2025-11-30T18:55:23.255393Z","shell.execute_reply":"2025-11-30T18:55:42.107359Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.11/dist-packages (0.5.0)\nRequirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.36.0)\nRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.2.6)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.3.0)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\nRequirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.19)\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.21.0+cu124)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.2.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.10.5)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.2.6)\nRequirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.3)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\nRequirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.2.6)\nRequirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.3.2)\nRequirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\nRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from nibabel) (2.2.6)\nRequirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (25.0)\nRequirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.15.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.2.6)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def prepare_kaggle_data():\n    \"\"\"–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç DataFrame —Å –ø—É—Ç—è–º–∏ –∫ –¥–∞–Ω–Ω—ã–º Kaggle\"\"\"\n    file_list = []\n    \n    kaggle_dirs = [\n        '/kaggle/input/liver-tumor-segmentation',\n        '/kaggle/input/liver-tumor-segmentation-part-2',\n        '../input/liver-tumor-segmentation',\n        '../input/liver-tumor-segmentation-part-2'\n    ]\n    \n    for base_dir in kaggle_dirs:\n        if os.path.exists(base_dir):\n            print(f\"‚úÖ Found directory: {base_dir}\")\n            for dirname, _, filenames in os.walk(base_dir):\n                for filename in filenames:\n                    if filename.endswith('.nii') or filename.endswith('.nii.gz'):\n                        file_list.append((dirname, filename))\n    \n    if not file_list:\n        print(\"‚ùå No NIfTI files found! Check directory paths.\")\n        return None\n    \n    df_files = pd.DataFrame(file_list, columns=['dirname', 'filename'])\n    df_files = df_files.sort_values(by=['filename'], ascending=True)\n    \n    print(f\"Total files found: {len(df_files)}\")\n    \n    # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Å–∫–∏\n    df_files[\"mask_dirname\"] = \"\"\n    df_files[\"mask_filename\"] = \"\"\n    \n    matched_count = 0\n    for i in range(131):\n        ct = f\"volume-{i}.nii\"\n        mask = f\"segmentation-{i}.nii\"\n        \n        ct_match = df_files['filename'] == ct\n        if ct_match.any():\n            df_files.loc[ct_match, 'mask_filename'] = mask\n            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø—É—Ç–∏ –¥–ª—è –º–∞—Å–æ–∫\n            possible_mask_dirs = [\n                \"../input/liver-tumor-segmentation/segmentations\",\n                \"/kaggle/input/liver-tumor-segmentation/segmentations\",\n                \"../input/liver-tumor-segmentation\",\n                \"/kaggle/input/liver-tumor-segmentation\"\n            ]\n            for mask_dir in possible_mask_dirs:\n                if os.path.exists(mask_dir):\n                    df_files.loc[ct_match, 'mask_dirname'] = mask_dir\n                    break\n            matched_count += 1\n    \n    print(f\"Matched {matched_count} volume-mask pairs\")\n    \n    # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏\n    df_volumes_with_masks = df_files[df_files['mask_filename'] != ''].copy()\n    df_volumes_with_masks = df_volumes_with_masks.reset_index(drop=True)\n    \n    df_volumes_with_masks['ct_path'] = df_volumes_with_masks['dirname'] + '/' + df_volumes_with_masks['filename']\n    df_volumes_with_masks['mask_path'] = df_volumes_with_masks['mask_dirname'] + '/' + df_volumes_with_masks['mask_filename']\n    \n    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤\n    existing_files = 0\n    for idx, row in df_volumes_with_masks.iterrows():\n        if os.path.exists(row['ct_path']) and os.path.exists(row['mask_path']):\n            existing_files += 1\n    \n    print(f\"Final dataset: {len(df_volumes_with_masks)} volumes with masks\")\n    print(f\"Files that exist: {existing_files}/{len(df_volumes_with_masks)}\")\n    \n    return df_volumes_with_masks\n\n# –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö\nprint(\"üîß Preparing Kaggle data...\")\ndf_kaggle = prepare_kaggle_data()\n\nif df_kaggle is not None:\n    print(\"‚úÖ Kaggle data loaded successfully!\")\n    df_kaggle.head()\nelse:\n    print(\"‚ùå Failed to load Kaggle data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:30:59.968378Z","iopub.execute_input":"2025-11-30T18:30:59.969166Z","iopub.status.idle":"2025-11-30T18:31:00.445531Z","shell.execute_reply.started":"2025-11-30T18:30:59.969139Z","shell.execute_reply":"2025-11-30T18:31:00.444768Z"}},"outputs":[{"name":"stdout","text":"üîß Preparing Kaggle data...\n‚úÖ Found directory: /kaggle/input/liver-tumor-segmentation\n‚úÖ Found directory: /kaggle/input/liver-tumor-segmentation-part-2\n‚úÖ Found directory: ../input/liver-tumor-segmentation\n‚úÖ Found directory: ../input/liver-tumor-segmentation-part-2\nTotal files found: 524\nMatched 131 volume-mask pairs\nFinal dataset: 262 volumes with masks\nFiles that exist: 262/262\n‚úÖ Kaggle data loaded successfully!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def read_nii(filepath):\n    \"\"\"–ß–∏—Ç–∞–µ—Ç .nii —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç numpy array\"\"\"\n    ct_scan = nib.load(filepath)\n    array = ct_scan.get_fdata()\n    array = np.rot90(np.array(array))\n    return array\n\ndef preprocess_and_save_data(df_files, output_dir='processed_data_deeplab_256', image_size=256):\n    \"\"\"\n    –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞ –¥–∏—Å–∫ –¥–ª—è DeepLabV3+\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(f'{output_dir}/images', exist_ok=True)\n    os.makedirs(f'{output_dir}/masks', exist_ok=True)\n    \n    image_paths = []\n    mask_paths = []\n    volume_ids = []\n    slice_ids = []\n    \n    print(f\"üöÄ Starting preprocessing for DeepLabV3+ - saving to {output_dir}\")\n    print(f\"üìê Target size: {image_size}x{image_size}\")\n    \n    for idx in tqdm(range(len(df_files)), desc=\"Processing volumes\"):\n        try:\n            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤\n            ct_path = df_files.loc[idx, 'ct_path']\n            mask_path = df_files.loc[idx, 'mask_path']\n            \n            if not os.path.exists(ct_path) or not os.path.exists(mask_path):\n                print(f\"‚ö†Ô∏è Files not found for volume {idx}\")\n                continue\n            \n            # –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n            ct_scan = read_nii(ct_path)\n            mask = read_nii(mask_path)\n            \n            # –ü–æ–ª—É—á–∞–µ–º ID volume –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞\n            file_name = df_files.loc[idx, 'filename']\n            volume_id = file_name.replace('volume-', '').replace('.nii', '')\n            \n            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï —Å—Ä–µ–∑—ã\n            num_slices = ct_scan.shape[2]\n            \n            for slice_idx in range(num_slices):\n                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ä–µ–∑\n                ct_slice = ct_scan[..., slice_idx]\n                mask_slice = mask[..., slice_idx]\n                \n                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è CT —Å—Ä–µ–∑–∞ [0, 1]\n                ct_slice_normalized = (ct_slice - ct_slice.min()) / (ct_slice.max() - ct_slice.min() + 1e-8)\n                \n                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ uint8 [0, 255] –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n                ct_slice_uint8 = (ct_slice_normalized * 255).astype(np.uint8)\n                mask_slice_uint8 = mask_slice.astype(np.uint8)\n                \n                # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞\n                ct_slice_resized = cv2.resize(ct_slice_uint8, (image_size, image_size))\n                mask_slice_resized = cv2.resize(mask_slice_uint8, (image_size, image_size), \n                                               interpolation=cv2.INTER_NEAREST)\n                \n                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n                image_filename = f'volume_{volume_id}_slice_{slice_idx:03d}.png'\n                mask_filename = f'volume_{volume_id}_slice_{slice_idx:03d}_mask.png'\n                \n                image_path = f'{output_dir}/images/{image_filename}'\n                mask_path = f'{output_dir}/masks/{mask_filename}'\n                \n                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ PNG\n                cv2.imwrite(image_path, ct_slice_resized)\n                cv2.imwrite(mask_path, mask_slice_resized)\n                \n                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç–∏\n                image_paths.append(image_path)\n                mask_paths.append(mask_path)\n                volume_ids.append(int(volume_id))\n                slice_ids.append(slice_idx)\n                \n        except Exception as e:\n            print(f\"‚ùå Error processing volume {idx}: {e}\")\n            continue\n    \n    # –°–æ–∑–¥–∞–µ–º DataFrame —Å –ø—É—Ç—è–º–∏\n    df_processed = pd.DataFrame({\n        'image_path': image_paths,\n        'mask_path': mask_paths,\n        'volume_id': volume_ids,\n        'slice_id': slice_ids\n    })\n    \n    # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV —Å –ø—É—Ç—è–º–∏\n    csv_path = f'{output_dir}/dataset.csv'\n    df_processed.to_csv(csv_path, index=False)\n    \n    print(f\"‚úÖ Preprocessing completed!\")\n    print(f\"üìÅ Saved {len(df_processed)} image-mask pairs\")\n    print(f\"üíæ CSV saved: {csv_path}\")\n    \n    return df_processed\n\n# –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É\nif df_kaggle is not None:\n    print(\"üîÑ Starting data preprocessing for DeepLabV3+...\")\n    df_processed = preprocess_and_save_data(df_kaggle, image_size=256)\n    print(\"‚úÖ Data preprocessing completed!\")\n    df_processed.head()\nelse:\n    print(\"‚ùå Cannot preprocess data - no Kaggle data loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:31:03.736073Z","iopub.execute_input":"2025-11-30T18:31:03.736727Z","iopub.status.idle":"2025-11-30T18:51:34.580402Z","shell.execute_reply.started":"2025-11-30T18:31:03.736695Z","shell.execute_reply":"2025-11-30T18:51:34.578996Z"}},"outputs":[{"name":"stdout","text":"üîÑ Starting data preprocessing for DeepLabV3+...\nüöÄ Starting preprocessing for DeepLabV3+ - saving to processed_data_deeplab_256\nüìê Target size: 256x256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing volumes:   0%|          | 0/262 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11db176a5dc24a8abc0fcdf13837511e"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Preprocessing completed!\nüìÅ Saved 117276 image-mask pairs\nüíæ CSV saved: processed_data_deeplab_256/dataset.csv\n‚úÖ Data preprocessing completed!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import segmentation_models_pytorch as smp\n\n# DeepLabV3+ Model Class\nclass DeepLabV3PlusModel:\n    def __init__(self, num_classes=3, encoder_name='resnet50', encoder_weights='imagenet'):\n        self.model = smp.DeepLabV3Plus(\n            encoder_name=encoder_name,\n            encoder_weights=encoder_weights,\n            classes=num_classes,\n            activation=None,\n            in_channels=1  # 1 –∫–∞–Ω–∞–ª –¥–ª—è CT —Å–Ω–∏–º–∫–æ–≤\n        )\n    \n    def to(self, device):\n        self.model = self.model.to(device)\n        return self\n    \n    def train(self):\n        self.model.train()\n    \n    def eval(self):\n        self.model.eval()\n    \n    def parameters(self):\n        return self.model.parameters()\n    \n    def state_dict(self):\n        return self.model.state_dict()\n    \n    def load_state_dict(self, state_dict):\n        self.model.load_state_dict(state_dict)\n\nprint(\"üß† Initializing DeepLabV3+ model...\")\ndeeplab_model = DeepLabV3PlusModel(num_classes=3, encoder_name='resnet50')\nprint(\"‚úÖ DeepLabV3+ model created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:53:52.536740Z","iopub.execute_input":"2025-11-30T18:53:52.537076Z","iopub.status.idle":"2025-11-30T18:54:01.235727Z","shell.execute_reply.started":"2025-11-30T18:53:52.537047Z","shell.execute_reply":"2025-11-30T18:54:01.234925Z"}},"outputs":[{"name":"stdout","text":"üß† Initializing DeepLabV3+ model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19e8c5ae61aa4c50b7b801e60121168d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19d29a86e0e64eb9b0a75302d92c9695"}},"metadata":{}},{"name":"stdout","text":"‚úÖ DeepLabV3+ model created!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nclass LiverDatasetDeepLab(Dataset):\n    def __init__(self, df, image_size=256, transform=None):\n        self.df = df\n        self.image_size = image_size\n        self.transform = transform\n        self.image_paths = df['image_path'].values\n        self.mask_paths = df['mask_path'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        # –ó–∞–≥—Ä—É–∑–∫–∞ –≥–æ—Ç–æ–≤—ã—Ö PNG\n        image_path = self.image_paths[idx]\n        mask_path = self.mask_paths[idx]\n        \n        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        \n        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è [0, 255] -> [0, 1]\n        image = image.astype(np.float32) / 255.0\n        \n        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–Ω–∞–ª—å–Ω–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ –¥–ª—è DeepLab\n        image = np.expand_dims(image, axis=0)  # [1, H, W]\n        \n        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã\n        image_tensor = torch.FloatTensor(image)  # [1, H, W]\n        mask_tensor = torch.LongTensor(mask)     # [H, W]\n        \n        return image_tensor, mask_tensor\n\ndef create_train_val_test_datasets(df_processed, image_size=256):\n    \"\"\"–°–æ–∑–¥–∞–µ—Ç train, validation –∏ test –¥–∞—Ç–∞—Å–µ—Ç—ã\"\"\"\n    \n    print(f\"üìä Creating train/val/test datasets from {len(df_processed)} samples\")\n    \n    # –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train+val (85%) –∏ test (15%)\n    train_val_df, test_df = train_test_split(\n        df_processed, test_size=0.15, random_state=42, shuffle=True\n    )\n    \n    # –ó–∞—Ç–µ–º —Ä–∞–∑–¥–µ–ª—è–µ–º train+val –Ω–∞ train (80%) –∏ val (20%)\n    train_df, val_df = train_test_split(\n        train_val_df, test_size=0.2, random_state=42, shuffle=True\n    )\n    \n    print(f\"üìä Training samples: {len(train_df)}\")\n    print(f\"üìä Validation samples: {len(val_df)}\")\n    print(f\"üìä Test samples: {len(test_df)}\")\n    \n    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã\n    train_dataset = LiverDatasetDeepLab(train_df, image_size=image_size)\n    val_dataset = LiverDatasetDeepLab(val_df, image_size=image_size)\n    test_dataset = LiverDatasetDeepLab(test_df, image_size=image_size)\n    \n    return train_dataset, val_dataset, test_dataset, train_df, val_df, test_df\n\n# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\nprint(\"üîÑ Creating train/val/test datasets...\")\ntrain_dataset_deeplab, val_dataset_deeplab, test_dataset_deeplab, train_df_deeplab, val_df_deeplab, test_df_deeplab = create_train_val_test_datasets(df_processed)\n\nprint(\"‚úÖ Datasets created!\")\n\ndef create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=8):\n    \"\"\"–°–æ–∑–¥–∞–µ—Ç DataLoader'—ã\"\"\"\n    \n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=2,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2,\n        pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2,\n        pin_memory=True\n    )\n    \n    print(f\"üìä DataLoaders created:\")\n    print(f\"  - Batch size: {batch_size}\")\n    print(f\"  - Training batches: {len(train_loader)}\")\n    print(f\"  - Validation batches: {len(val_loader)}\")\n    print(f\"  - Test batches: {len(test_loader)}\")\n    \n    return train_loader, val_loader, test_loader\n\n# –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤\nprint(\"üîÑ Creating DataLoaders...\")\ntrain_loader_deeplab, val_loader_deeplab, test_loader_deeplab = create_dataloaders(\n    train_dataset_deeplab, val_dataset_deeplab, test_dataset_deeplab, batch_size=8\n)\nprint(\"‚úÖ DataLoaders ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:55:10.853821Z","iopub.execute_input":"2025-11-30T18:55:10.854417Z","iopub.status.idle":"2025-11-30T18:55:12.724038Z","shell.execute_reply.started":"2025-11-30T18:55:10.854389Z","shell.execute_reply":"2025-11-30T18:55:12.723036Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_183/1013406283.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLiverDatasetDeepLab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_output\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_SetOutputMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m from .utils._tags import (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32msklearn/utils/murmurhash.pyx\u001b[0m in \u001b[0;36minit sklearn.utils.murmurhash\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"],"ename":"ValueError","evalue":"numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"def create_dataset_csv(processed_dir='processed_data_deeplab_256'):\n    \"\"\"\n    –°–æ–∑–¥–∞–µ—Ç CSV —Ñ–∞–π–ª —Å –ø—É—Ç—è–º–∏ –∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º\n    \"\"\"\n    images_dir = os.path.join(processed_dir, 'images')\n    masks_dir = os.path.join(processed_dir, 'masks')\n    \n    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π\n    if not os.path.exists(images_dir) or not os.path.exists(masks_dir):\n        print(f\"‚ùå Processed directories not found: {images_dir} or {masks_dir}\")\n        return None\n    \n    image_paths = []\n    mask_paths = []\n    volume_ids = []\n    slice_ids = []\n    \n    print(f\"üîç Scanning existing processed data in {processed_dir}\")\n    \n    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n    image_files = [f for f in os.listdir(images_dir) if f.endswith('.png') and not f.endswith('_mask.png')]\n    \n    for image_file in tqdm(image_files, desc=\"Processing existing files\"):\n        try:\n            # –ü–∞—Ä—Å–∏–º –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è volume_id –∏ slice_id\n            # –§–æ—Ä–º–∞—Ç: volume_{volume_id}_slice_{slice_idx}.png\n            base_name = image_file.replace('.png', '')\n            parts = base_name.split('_')\n            \n            if len(parts) >= 4 and parts[0] == 'volume' and parts[2] == 'slice':\n                volume_id = parts[1]\n                slice_id = parts[3]\n                \n                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç–∏\n                image_path = os.path.join(images_dir, image_file)\n                mask_filename = f'volume_{volume_id}_slice_{slice_id}_mask.png'\n                mask_path = os.path.join(masks_dir, mask_filename)\n                \n                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º–∞—Å–∫–∏\n                if os.path.exists(mask_path):\n                    image_paths.append(image_path)\n                    mask_paths.append(mask_path)\n                    volume_ids.append(int(volume_id))\n                    slice_ids.append(int(slice_id))\n                else:\n                    print(f\"‚ö†Ô∏è Mask not found: {mask_filename}\")\n                    \n        except Exception as e:\n            print(f\"‚ùå Error processing file {image_file}: {e}\")\n            continue\n    \n    # –°–æ–∑–¥–∞–µ–º DataFrame\n    df_processed = pd.DataFrame({\n        'image_path': image_paths,\n        'mask_path': mask_paths,\n        'volume_id': volume_ids,\n        'slice_id': slice_ids\n    })\n    \n    # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV\n    csv_path = os.path.join(processed_dir, 'dataset.csv')\n    df_processed.to_csv(csv_path, index=False)\n    \n    print(f\"‚úÖ Dataset CSV created!\")\n    print(f\"üìä Found {len(df_processed)} image-mask pairs\")\n    print(f\"üíæ CSV saved: {csv_path}\")\n    \n    return df_processed\n\n# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è - –ø—Ä–æ—Å—Ç–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π CSV –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å\ndef load_existing_dataset(processed_dir='processed_data_deeplab_256'):\n    \"\"\"\n    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ CSV —Ñ–∞–π–ª–∞\n    \"\"\"\n    csv_path = os.path.join(processed_dir, 'dataset.csv')\n    \n    if os.path.exists(csv_path):\n        print(f\"üìÅ Loading existing dataset from {csv_path}\")\n        df_processed = pd.read_csv(csv_path)\n        print(f\"‚úÖ Loaded {len(df_processed)} image-mask pairs\")\n        return df_processed\n    else:\n        print(f\"üìÅ CSV not found, creating new one...\")\n        return create_dataset_csv(processed_dir)\n\n# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\nprint(\"üîÑ Checking for existing processed data...\")\n\n# –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ\ndf_processed = load_existing_dataset('processed_data_deeplab_256')\n\nif df_processed is not None and len(df_processed) > 0:\n    print(\"‚úÖ Successfully loaded processed dataset!\")\n    print(f\"üìä Dataset info: {len(df_processed)} samples\")\n    print(df_processed.head())\nelse:\n    print(\"‚ùå No processed data found. You need to run preprocessing first.\")\n    print(\"To preprocess data, run:\")\n    print(\"df_processed = preprocess_and_save_data(df_kaggle, image_size=256)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:54:44.561880Z","iopub.execute_input":"2025-11-30T18:54:44.562452Z","iopub.status.idle":"2025-11-30T18:54:44.790206Z","shell.execute_reply.started":"2025-11-30T18:54:44.562428Z","shell.execute_reply":"2025-11-30T18:54:44.789282Z"}},"outputs":[{"name":"stdout","text":"üîÑ Checking for existing processed data...\nüìÅ Loading existing dataset from processed_data_deeplab_256/dataset.csv\n‚úÖ Loaded 117276 image-mask pairs\n‚úÖ Successfully loaded processed dataset!\nüìä Dataset info: 117276 samples\n                                          image_path  \\\n0  processed_data_deeplab_256/images/volume_0_sli...   \n1  processed_data_deeplab_256/images/volume_0_sli...   \n2  processed_data_deeplab_256/images/volume_0_sli...   \n3  processed_data_deeplab_256/images/volume_0_sli...   \n4  processed_data_deeplab_256/images/volume_0_sli...   \n\n                                           mask_path  volume_id  slice_id  \n0  processed_data_deeplab_256/masks/volume_0_slic...          0         0  \n1  processed_data_deeplab_256/masks/volume_0_slic...          0         1  \n2  processed_data_deeplab_256/masks/volume_0_slic...          0         2  \n3  processed_data_deeplab_256/masks/volume_0_slic...          0         3  \n4  processed_data_deeplab_256/masks/volume_0_slic...          0         4  \n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def calculate_metrics(preds, targets, num_classes=3):\n    \"\"\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫: IoU –∏ Dice –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞\"\"\"\n    ious = []\n    dices = []\n    preds = torch.argmax(preds, dim=1)\n    \n    for cls in range(num_classes):\n        pred_inds = (preds == cls)\n        target_inds = (targets == cls)\n        \n        intersection = (pred_inds & target_inds).float().sum()\n        union = (pred_inds | target_inds).float().sum()\n        \n        # IoU\n        if union == 0:\n            ious.append(float('nan'))\n        else:\n            ious.append(float(intersection / union))\n        \n        # Dice\n        if (pred_inds.float().sum() + target_inds.float().sum()) == 0:\n            dices.append(float('nan'))\n        else:\n            dice = (2. * intersection) / (pred_inds.float().sum() + target_inds.float().sum())\n            dices.append(float(dice))\n    \n    return np.nanmean(ious), np.nanmean(dices)\n\ndef train_deeplabv3plus(model, train_loader, val_loader, num_epochs=15, device='cuda'):\n    \"\"\"–û–±—É—á–µ–Ω–∏–µ DeepLabV3+ –º–æ–¥–µ–ª–∏\"\"\"\n    \n    from torch.optim import Adam\n    from torch.optim.lr_scheduler import ReduceLROnPlateau\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n    \n    model.to(device)\n    \n    # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n    train_losses = []\n    val_losses = []\n    val_ious = []\n    val_dices = []\n    \n    best_iou = 0.0\n    best_dice = 0.0\n    \n    print(f\"üöÄ Starting DeepLabV3+ training for {num_epochs} epochs...\")\n    print(f\"üìä Training on {len(train_loader.dataset)} samples\")\n    print(f\"üìä Validating on {len(val_loader.dataset)} samples\")\n    print(\"=\" * 60)\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        start_time = time.time()\n        \n        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n        for batch_idx, (images, masks) in enumerate(train_pbar):\n            images, masks = images.to(device), masks.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model.model(images)\n            loss = criterion(outputs, masks)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            train_pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        epoch_val_ious = []\n        epoch_val_dices = []\n        \n        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n        with torch.no_grad():\n            for images, masks in val_pbar:\n                images, masks = images.to(device), masks.to(device)\n                outputs = model.model(images)\n                \n                loss = criterion(outputs, masks)\n                val_loss += loss.item()\n                \n                iou, dice = calculate_metrics(outputs, masks)\n                epoch_val_ious.append(iou)\n                epoch_val_dices.append(dice)\n                \n                val_pbar.set_postfix({\n                    'Loss': f'{loss.item():.4f}', \n                    'mIoU': f'{iou:.4f}',\n                    'Dice': f'{dice:.4f}'\n                })\n        \n        # Calculate epoch metrics\n        epoch_train_loss = running_loss / len(train_loader)\n        epoch_val_loss = val_loss / len(val_loader)\n        epoch_val_iou = np.mean(epoch_val_ious)\n        epoch_val_dice = np.mean(epoch_val_dices)\n        epoch_time = time.time() - start_time\n        \n        train_losses.append(epoch_train_loss)\n        val_losses.append(epoch_val_loss)\n        val_ious.append(epoch_val_iou)\n        val_dices.append(epoch_val_dice)\n        \n        scheduler.step(epoch_val_loss)\n        \n        # Print epoch results\n        print(f'\\n‚úÖ Epoch {epoch+1} Complete:')\n        print(f'   ‚è±Ô∏è  Time: {epoch_time/60:.1f} min')\n        print(f'   üìâ Train Loss: {epoch_train_loss:.4f}')\n        print(f'   üìä Val Loss: {epoch_val_loss:.4f}')\n        print(f'   üéØ mIoU: {epoch_val_iou:.4f}')\n        print(f'   üéØ Dice: {epoch_val_dice:.4f}')\n        print(f'   üìö LR: {optimizer.param_groups[0][\"lr\"]:.2e}')\n        \n        # Save best model\n        if epoch_val_iou > best_iou:\n            best_iou = epoch_val_iou\n            best_dice = epoch_val_dice\n            torch.save(model.state_dict(), 'best_deeplabv3plus_liver.pth')\n            print(f'   üíæ Best model saved! (IoU: {best_iou:.4f}, Dice: {best_dice:.4f})')\n        \n        print('-' * 50)\n    \n    # Save final model\n    torch.save(model.state_dict(), 'final_deeplabv3plus_liver.pth')\n    print(f\"üéâ Training completed! Best IoU: {best_iou:.4f}\")\n    \n    return {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_ious': val_ious,\n        'val_dices': val_dices,\n        'best_iou': best_iou,\n        'best_dice': best_dice\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:54:49.461229Z","iopub.execute_input":"2025-11-30T18:54:49.461916Z","iopub.status.idle":"2025-11-30T18:54:49.475846Z","shell.execute_reply.started":"2025-11-30T18:54:49.461890Z","shell.execute_reply":"2025-11-30T18:54:49.475205Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T17:33:07.069155Z","iopub.execute_input":"2025-11-30T17:33:07.069805Z","iopub.status.idle":"2025-11-30T17:33:07.093516Z","shell.execute_reply.started":"2025-11-30T17:33:07.069778Z","shell.execute_reply":"2025-11-30T17:33:07.092651Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3819112499.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mdf_kaggle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üîÑ Starting data preprocessing for DeepLabV3+...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mdf_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_and_save_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_kaggle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_kaggle' is not defined"],"ename":"NameError","evalue":"name 'df_kaggle' is not defined","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üñ•Ô∏è Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"üéØ GPU: {torch.cuda.get_device_name()}\")\n    print(f\"üéØ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n\n# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\nprint(\"üî• Starting DeepLabV3+ Training...\")\nprint(\"=\" * 50)\n\nresults_deeplab = train_deeplabv3plus(\n    model=deeplab_model,\n    train_loader=train_loader_deeplab,\n    val_loader=val_loader_deeplab,\n    num_epochs=15,\n    device=device\n)\n\nprint(\"üéâ DeepLabV3+ training completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:54:54.856166Z","iopub.execute_input":"2025-11-30T18:54:54.856412Z","execution_failed":"2025-11-30T18:54:54.164Z"}},"outputs":[{"name":"stdout","text":"üñ•Ô∏è Using device: cuda\nüéØ GPU: Tesla T4\nüéØ GPU Memory: 14.7 GB\nüî• Starting DeepLabV3+ Training...\n==================================================\nüöÄ Starting DeepLabV3+ training for 15 epochs...\nüìä Training on 79747 samples\nüìä Validating on 19937 samples\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/15 [Train]:   0%|          | 0/9969 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"013d5cf088c94c6dbdfaea088c963371"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"def plot_training_results(results):\n    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è\"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # Loss\n    axes[0, 0].plot(results['train_losses'], label='Train Loss', linewidth=2, color='blue')\n    axes[0, 0].plot(results['val_losses'], label='Val Loss', linewidth=2, color='red')\n    axes[0, 0].set_title('Training and Validation Loss')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True)\n    \n    # mIoU\n    axes[0, 1].plot(results['val_ious'], label='mIoU', linewidth=2, color='green')\n    axes[0, 1].set_title('Validation mIoU')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('mIoU')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True)\n    \n    # Dice\n    axes[1, 0].plot(results['val_dices'], label='Dice', linewidth=2, color='orange')\n    axes[1, 0].set_title('Validation Dice Coefficient')\n    axes[1, 0].set_xlabel('Epoch')\n    axes[1, 0].set_ylabel('Dice')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True)\n    \n    # Combined metrics\n    axes[1, 1].plot(results['val_ious'], label='mIoU', linewidth=2, color='green')\n    axes[1, 1].plot(results['val_dices'], label='Dice', linewidth=2, color='orange')\n    axes[1, 1].set_title('Validation Metrics Comparison')\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].set_ylabel('Score')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print final results\n    print(f\"\\nüèÜ FINAL DEEPLABV3+ RESULTS:\")\n    print(\"=\" * 40)\n    print(f\"Best mIoU: {results['best_iou']:.4f}\")\n    print(f\"Best Dice: {results['best_dice']:.4f}\")\n    print(f\"Final mIoU: {results['val_ious'][-1]:.4f}\")\n    print(f\"Final Dice: {results['val_dices'][-1]:.4f}\")\n\n# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\nprint(\"üìä Plotting training results...\")\nplot_training_results(results_deeplab)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}